### <center>IEMS 5730 Spring 2019 Homework 4</center>
<center><b>Name:&nbsp</b> <u>Wenli SONG</u></center>
<center><b>Student No.:&nbsp</b><u>1155114524</u></center><br>

*I declare that the assignment submitted on Elearning system is original except for source material explicitly acknowledged, and that the same or related material has not been previously submitted for another course. I also acknowledge that I am aware of University policy and regulations on honesty in academic work, and of the disciplinary guidelines and procedures applicable to breaches of such policy and regulations, as contained in the website [http://www.cuhk.edu.hk/policy/academichonesty/​](http://www.cuhk.edu.hk/policy/academichonesty/​).*

Signed (Student_________________________) Date:______________________________
Name___________________________________ SID_______________________________

#### Q1: Spark Basic RDD
**Multi-node Spark Cluster Setup**
Install scala and Spark
```
sudo apt-get install scala
scala -version
wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
tar xzf spark-2.4.0-bin-hadoop2.7.tgz
mv spark-2.4.0-bin-hadoop2.7 ~/spark
```
Confiure Spark `vi ~/.bashrc`
```
#Spark
export SPARK_HOME=/home/ubuntu/spark
export PATH=$SPARK_HOME/bin:$PATH
```
`vi ~/spark/conf/spark-env.sh`
```
export SPARK_MASTER_HOST=ip-172-31-13-50
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```
`vi ~/spark/conf/spark-defaults.conf`
```
spark.master                     spark://ip-172-31-13-50:7077
```
`vi ~/spark/conf/slaves`
```
ip-172-31-13-50
ip-172-31-15-23
ip-172-31-1-146
ip-172-31-0-227
```
Start Spark Cluster `~/spark/sbin/start-all.sh`
And we can see on the \<master-node-ip>:8080
<image src=./spark.png/>

**(a) Naive implementation of PageRank**
Start a project
```
mvn archetype:generate
net.alchim31.maven:scala-archetype-simple
```
Code
```
package rddpr

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

/**
 * @author ${user.name}
 */
object PageRank {
  
  def main(args : Array[String]) {
    val conf = new SparkConf().setAppName("pagerank").setMaster("yarn")
    val sc = new SparkContext(conf)
    val lines = sc.textFile("hdfs:///user/ubuntu/web-Google.txt")
    val tmp_links = lines.map(line => {
                              val arr = line.split("\\s+")
                              (arr(0),arr(1))
                    })
    val links = tmp_links.distinct().groupByKey().cache()
    var ranks = links.mapValues(v => 1.0)
    for (i<- 1 to 30){
      val contribs = links.join(ranks).flatMap{
        case (start, (ends,rank)) =>
          ends.map(end => (end, rank/ends.size))
        }
      ranks = contribs.reduceByKey(_+_)
                      .mapValues(0.15+0.85*_)
    }
    ranks.sortBy(_._2, false).saveAsTextFile("hdfs:///user/ubuntu/pagerank-output-1")
  }
}
```
Download dataset and put it into hdfs
Package and submit to cluster
Output result
```
wget https://snap.stanford.edu/data/web-Google.txt.gz
gunzip web-Google.txt.gz
hdfs dfsadmin -safemode leave
hdfs dfs -put web-Google.txt .
mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies
hdfs dfs -rm -r pagerank*
~/spark/bin/spark-submit \
  --class rddpr.PageRank \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 1G \
  --num-executors 24 \
  ./target/rddpr-1.0-SNAPSHOT-jar-with-dependencies.jar \
  1000

hdfs dfs -get pagerank-output-1 .
head -n 100 pagerank-output-1/part-00000
```
Top 100 nodes
```
(41909,393.64784049851556)
(504140,372.18133921956553)
(597621,364.51089799415996)
(384666,357.87552330599823)
(486980,348.92688952000066)
(537039,340.55812781839506)
(751384,338.28736328063957)
(32163,336.22398293322385)
(765334,328.4642681533871)
(605856,328.01102240393396)
(163075,316.95853621095966)
(425770,308.32800008516574)
(558791,307.2697714280831)
(173976,305.0807906433802)
(908351,295.32306965765366)
(459074,277.8939148849685)
(905628,276.02583882293214)
(687325,266.1354369227145)
(452291,263.44185776229904)
(396321,263.19268006622906)
(172133,263.02979856884707)
(226374,259.78016455141073)
(551829,259.14731525516515)
(407610,258.2536315961823)
(828963,255.55628359746822)
(885605,244.19971044708817)
(7314,239.5201081819956)
(691633,236.11067547076115)
(182121,233.9198632653493)
(726490,231.19389949462064)
(213432,228.31537452706561)
(804489,227.10503824259837)
(324502,224.87711025511413)
(721816,221.50200796579736)
(245186,215.25178595527254)
(185067,214.1579529941636)
(402132,212.50537334423166)
(54147,209.18283792138618)
(599130,208.88971259341866)
(277876,207.93551560153625)
(185821,207.92562902758883)
(587935,203.7896442221374)
(399699,202.19572577754911)
(621670,200.7397404081141)
(448961,196.50127842271738)
(57791,194.46064653118958)
(191990,193.95683326957771)
(555924,192.2385278313085)
(870811,190.21808410573132)
(434134,188.1269679510054)
(699754,187.03372787859354)
(614831,183.66994035806152)
(715883,181.46512093545098)
(468736,181.0508054819036)
(323129,180.6308093417644)
(486434,178.12838811326975)
(818382,174.0968530541037)
(596972,171.731026329631)
(626893,171.2325724632065)
(354640,167.93155443660586)
(846221,167.64278877678754)
(74397,161.2981216693098)
(637936,160.099765592587)
(183501,160.050389166975)
(1536,159.51087462414839)
(557124,158.6085569948089)
(587721,157.43592684031313)
(223236,157.15271797462458)
(466323,155.39021741564923)
(819223,154.78825205110255)
(666882,154.35222347072215)
(483532,153.7324401186006)
(21682,153.32619120593665)
(138746,150.01934591496638)
(227675,147.92219630680015)
(812640,145.7056105714577)
(772466,144.72941332094132)
(369102,142.50718253030124)
(438493,141.53009427571806)
(187242,139.19965421548687)
(427629,137.52000565408287)
(495084,136.80680713470707)
(798584,136.28229200496196)
(298511,135.83112354324098)
(522190,135.6338651582835)
(133962,131.95434622714984)
(704396,127.70661257171037)
(48888,126.44008839212327)
(36292,122.53020253309998)
(887710,121.22616637604574)
(564521,121.15096569890925)
(438491,120.76645176234244)
(655158,118.6006744856297)
(673065,117.397130714686)
(539241,116.3465698576141)
(352975,116.06082058771874)
(456932,114.27682001840503)
(568104,112.83971034450705)
(556129,112.80889168531188)
(762917,109.55905733778206)
```
**(b) Advanced implementation of PageRank(pre-partition mechanism)**
|Number of partitions|Time cost|
|-|-|
|No pre-partition|8 mins 58 s|
|10|2 mins 21 s|
|25|2 mins 13 s|
|50|2 mins 32 s|
|75|5 mins 43 s|
|100|5 mins 48 s|

#### Q2: Spark SQL
**(a) Use Spark to truncate the file and only keep (​CCN, REPORTDATETIME, OFFENSE, METHOD, LASTMODIFIEDDATE, DISTRICT​) of each line of record**
Download dataset and put it into hdfs
```
wget -O crime_incidents.csv \
https://opendata.arcgis.com/datasets/5fa2e43557f7484d89aac9e1e76158c9_10.csv
hdfs dfs -put crime_incidents.csv .
```
Open spark shell `~/spark/bin/spark-shell.sh`
```
val df = spark.read.format("csv").option("header", "true")
                  .option("mode", "DROPMALFORMED")
                  .load("hdfs:///user/ubuntu/crime_incidents.csv")
val mydf = df.select("CCN","REPORT_DAT","OFFENSE","METHOD","SHIFT","DISTRICT").na.drop
mydf.show()
+--------+--------------------+--------------------+------+--------+--------+
|     CCN|          REPORT_DAT|             OFFENSE|METHOD|   SHIFT|DISTRICT|
+--------+--------------------+--------------------+------+--------+--------+
|04104147|2013-04-16T00:00:...|            HOMICIDE| KNIFE|MIDNIGHT|       1|
|09251354|2013-02-27T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       2|
|10028985|2013-02-27T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       5|
|10033521|2013-10-10T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       6|
|11010107|2013-07-31T00:00:...|            HOMICIDE|OTHERS|MIDNIGHT|       5|
|11045512|2013-01-31T00:00:...|            HOMICIDE|   GUN|MIDNIGHT|       6|
|11250281|2013-07-08T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       6|
|12003591|2013-01-09T00:59:...|         THEFT/OTHER|OTHERS|MIDNIGHT|       5|
|12037530|2013-03-23T10:00:...|         THEFT/OTHER|OTHERS|     DAY|       2|
|12055744|2013-08-19T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       3|
|12128314|2013-09-10T18:43:...|            BURGLARY|OTHERS| EVENING|       3|
|12139462|2013-11-13T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       1|
|12174206|2013-08-26T00:00:...|           SEX ABUSE|OTHERS|MIDNIGHT|       3|
|12182426|2013-01-01T00:15:...|             ROBBERY|   GUN|MIDNIGHT|       6|
|12182429|2013-01-01T00:50:...|            BURGLARY|OTHERS|MIDNIGHT|       1|
|12182465|2013-01-01T00:18:...|             ROBBERY|   GUN|MIDNIGHT|       6|
|12182466|2013-01-01T02:10:...|        THEFT F/AUTO|OTHERS|MIDNIGHT|       7|
|12182489|2013-01-01T01:28:...|ASSAULT W/DANGERO...| KNIFE|MIDNIGHT|       5|
|12182502|2013-01-01T03:00:...|         THEFT/OTHER|OTHERS|MIDNIGHT|       1|
|12182505|2013-01-01T02:29:...|             ROBBERY|   GUN|MIDNIGHT|       4|
+--------+--------------------+--------------------+------+--------+--------+
only showing top 20 rows
```
**(b)  count the number of each type offenses and find which time-slot (shift) did the most crimes occur**
```
<!-- import org.apache.spark.sql.functions.countDistinct -->
<!-- mydf.agg(countDistinct("OFFENSE")).show() -->
mydf.groupBy("OFFENSE").count().show()
+-----------------------+ 
|count(DISTINCT OFFENSE)|
+-----------------------+
|                      9|
+-----------------------+

mydf.groupBy("SHIFT").count().show()
+--------+-----+
|   SHIFT|count|
+--------+-----+
|MIDNIGHT| 6512|
| EVENING|15035|
|     DAY|14291|
+--------+-----+
```

**(c)**
```
wget -O data2010.csv https://opendata.arcgis.com/datasets/fdacfbdda7654e06a161352247d3a2f0_34.csv
wget -O data2011.csv https://opendata.arcgis.com/datasets/9d5485ffae914c5f97047a7dd86e115b_35.csv
wget -O data2012.csv https://opendata.arcgis.com/datasets/010ac88c55b1409bb67c9270c8fc18b5_11.csv
wget -O data2013.csv https://opendata.arcgis.com/datasets/5fa2e43557f7484d89aac9e1e76158c9_10.csv
wget -O data2014.csv https://opendata.arcgis.com/datasets/6eaf3e9713de44d3aa103622d51053b5_9.csv
wget -O data2015.csv https://opendata.arcgis.com/datasets/35034fcb3b36499c84c94c069ab1a966_27.csv
wget -O data2016.csv https://opendata.arcgis.com/datasets/bda20763840448b58f8383bae800a843_26.csv
wget -O data2017.csv https://opendata.arcgis.com/datasets/6af5cb8dc38e4bcbac8168b27ee104aa_38.csv
wget -O data2018.csv https://opendata.arcgis.com/datasets/38ba41dd74354563bce28a359b59324e_0.csv
hdfs dfs -put data201*.csv .

val df = spark.read.format("csv").option("header", "true").load("hdfs:///user/ubuntu/data201*.csv")
val total = df.groupBy(year($"REPORT_DAT").alias("year")).count().withColumnRenamed("count", "total")
val gun = df.filter($"METHOD".equalTo("GUN")).groupBy(year($"REPORT_DAT").alias("year")).count()

gun.join(total,"year").withColumn("percentage", $"count"/$"total").orderBy($"year".desc).show()
+----+-----+-----+--------------------+                                         
|year|count|total|          percentage|
+----+-----+-----+--------------------+
|2018| 1593|33641| 0.04735293243363753|
|2017| 1585|33080| 0.04791414752116082|
|2016| 2124|37199|0.057098309094330495|
|2015| 2189|37294| 0.05869576875636832|
|2014| 1960|38396| 0.05104698406083967|
|2013| 2203|35861|0.061431638827695825|
|2012| 2206|35280| 0.06252834467120182|
|2011| 1853|33264|  0.0557058682058682|
|2010| 2015|31642| 0.06368118323746919|
+----+-----+-----+--------------------+
```
From the table we can see that the percentage of gun offense dropped about 1% after Obama’s executive actions on gun control.